val x = spark.read.json("file:///home/ubuntu/sparkfile/people.json")

x.show()

x.printSchema()

x.select($"name",$"age").show()

x.filter($"age" > 20).show()



val mydata = spark.read.format("csv").option("inferSchema","true").option("header","true").load("file:///home/ubuntu/sparkfile/Bank_full.csv")

mydata.printSchema()

mydata.show(50)

val totcount = mydata.count.toDouble

val suscount = mydata.filter($"y" === "yes").count.toDouble

val successrate = suscount/totcount * 100

mydata.select(avg($"balance")).show()

mydata.createOrReplaceTempView("bankdata")

val x= spark.sql("select * from bankdata where age > 74")

x.show()

x.write.save("mynewdf")


x.write.format("json").save("mynewdf1")


------------

x = sc.textFile("/home/ubuntu/sparkfile/people.txt")

y = x.map(lambda n: n.upper())

y.collect()

a = ('hello','world','cool','winter','summer')
b = ['hello','world','cool','winter','summer']

type(a)

c = sc.parallelize(b)
c.count()


y.getNumPartitions()


import pyspark

b = ['hello','world','cool','winter','summer']

c = sc.parallelize(b,2).persist(pyspark.StorageLevel.MEMORY_ONLY)
c.count()



